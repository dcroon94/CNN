{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Learning Machine preparation\n",
    "\n",
    "Author: Dennis Croon\n",
    "Date:   5-dec-2019\n",
    "Source: https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia#person100_bacteria_475.jpeg\n",
    "\n",
    "This report is made to prepare the image dataset for the SLM, this means converting x-ray pixels to clear feature arrays. The dataset is a public Kaggle dataset and contains information about Pneumonia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"/Users/denniscroon/Desktop/Thesis/vgg_feature_extraction-master/data/train/PNEUMONIA/person8_bacteria_37.jpeg\",width =300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x-ray photo above is an example of Pneumonia, all the images have this point of view, but some of them with a slightly change around the lungs. Unfortunately the data does not have a consistency about size; which means all the photos have different height and width. There is a third labelled distinguish possible, namely inside the Pneumonia cases: they could be either bacterial or viral. But for now on it is kept binary.\n",
    "\n",
    "After exploring the data, it is time to extract the features from each image. Instead of having an array of 2000x2000 (mean pixels) with each cell filled with a value between 0 and 255 (darkness), we rather have an array that points out the specific properties of each image. To make this a bit faster and smarter, we use the knowledge of the pre-trained model VGG16 (expansion and comparison with other models possible). The determination of the features are combined with the change to jsonl format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = VGG16(weights='imagenet', include_top=False)\n",
    "    image_paths = [x for x in Path(\"data/\").glob('**/*') if x.is_file() and \".jpeg\" in x.name]\n",
    "    output = []\n",
    "    for fpath in tqdm(image_paths, desc=\"Extracting Features\", total=len(image_paths)):\n",
    "        split_path = str(fpath).split(\"/\")  \n",
    "        img = image.load_img(fpath, target_size=(224,224))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        features = model.predict(x)\n",
    "        output.append({\n",
    "            \"file_path\": str(fpath),\n",
    "            \"dataset\": split_path[1],\n",
    "            \"label\": split_path[2],\n",
    "            \"file_name\": split_path[3],\n",
    "            \"features\": features.tolist()\n",
    "        })\n",
    "    with jsonlines.open(\"features.jsonl\", \"w\") as outfile:\n",
    "        outfile.write_all(output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the feature.jsonl file is created for the train and test dataset, a load-method is necessary for future purposes. Here we can convert the jsonl format to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def main():\n",
    "    num_lines = sum(1 for line in open('features.jsonl'))\n",
    "    with jsonlines.open(\"features.jsonl\") as jsonl_fh:\n",
    "        train, test = [], []\n",
    "\n",
    "        for obj in tqdm(jsonl_fh, desc=\"Loading Features\", total=num_lines):\n",
    "            if obj[\"dataset\"] == \"train\":\n",
    "                train.append(obj)\n",
    "            elif obj[\"dataset\"] == \"test\":\n",
    "                test.append(obj)\n",
    "\n",
    "    train_df = pd.DataFrame.from_records(train)\n",
    "    test_df = pd.DataFrame.from_records(test)\n",
    "\n",
    "    print(train_df.head())\n",
    "    print(test_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset                                           features  \\\n",
    "0   train  [[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....   \n",
    "1   train  [[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....   \n",
    "2   train  [[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....   \n",
    "3   train  [[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....   \n",
    "4   train  [[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....   \n",
    "\n",
    "                   file_name                                    file_path  \\\n",
    "0  NORMAL2-IM-0867-0001.jpeg  data/train/NORMAL/NORMAL2-IM-0867-0001.jpeg   \n",
    "1  NORMAL2-IM-0903-0001.jpeg  data/train/NORMAL/NORMAL2-IM-0903-0001.jpeg   \n",
    "2          IM-0691-0001.jpeg          data/train/NORMAL/IM-0691-0001.jpeg   \n",
    "3  NORMAL2-IM-0395-0001.jpeg  data/train/NORMAL/NORMAL2-IM-0395-0001.jpeg   \n",
    "4     IM-0650-0001-0001.jpeg     data/train/NORMAL/IM-0650-0001-0001.jpeg   \n",
    "\n",
    "    label  \n",
    "0  NORMAL  \n",
    "1  NORMAL  \n",
    "2  NORMAL  \n",
    "3  NORMAL  \n",
    "4  NORMAL  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
